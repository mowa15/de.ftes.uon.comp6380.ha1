# -*- coding: utf-8 -*-
"""
Created on Thu Mar 26 15:30:21 2015

@author: Moritz
"""

#training parameters for neural networks:
learning_rate = 0.05 #set in [0,1]

learning_decay = 0.999 #try 0.999, set in [0.9,1]

momentum = 0.01 # set in [0,0.5]

batch_learning = False #set to learn in batches

validation_proportion = 0. # set in [0,0.5]

hidden_layers = [20] #number of neurons in each hidden layer, make as many layers as you feel like. Try increasing this to 10

iterations = 15 #used only if validaton proportion is 0

#include/import the libraries we need for loading CSV files -------------------------------------------------------------------------------------------------------------------
from pybrain.datasets import SupervisedDataSet
from pybrain.utilities import one_to_n

def loadCSV(filename,multiclass=True,outputs=1,separator=','):
    #read in all the lines
    f = open(filename).readlines()
    
    #start our datasets
    in_data = []
    out_data =[]
    
    #process the file
    for line in f:
        #remove whitespace and split according to separator character
        samples = line.strip(' \r\n').split(separator)
        
        #save input data
        in_data.append([float(i) for i in samples[:-outputs]])
        
        #save output data
        if multiclass:
            out_data.append(samples[-1])
        else:
            out_data.append([float(i) for i in samples[-outputs:]])
        
    
    processed_out_data = out_data
    
    #process multiclass encoding
    if multiclass:
        processed_out_data = []
        #get all the unique values for classes
        keys = []
        for d in out_data:
            if d not in keys:
                keys.append(d)
        keys.sort()
        #encode all data
        for d in out_data:
            processed_out_data.append(one_to_n(keys.index(d),len(keys)))
    
    #create the dataset
    dataset = SupervisedDataSet(len(in_data[0]),len(processed_out_data[0]))
    for i in xrange(len(out_data)):
        dataset.addSample(in_data[i],processed_out_data[i])
    
    #return the keys if we have
    if multiclass:
        return dataset,keys # a multiclass classifier
    else:
        return dataset
        
        
     #train the neural network ---------------------------------------------------------------------------------------------------------------------------------------------------
from pybrain.tools.shortcuts import buildNetwork
from pybrain.supervised.trainers import BackpropTrainer
#from pybrain.supervised.trainers.rprop import RPropMinusTrainer
#from pybrain.supervised.trainers.evolino import EvolinoTrainer

data,keys = loadCSV("subject.csv")
#nn = buildNetwork(*([data.indim]+hidden_layers+[data.outdim]))
nn = buildNetwork(561,hidden_layers,1)
   
   
   
   #Training a Neural Net
#include/import supervised dataset object
from pybrain.datasets import SupervisedDataSet
#create a dataset with 2 inputs and 1 output
trainer = BackpropTrainer(nn,data)
#train for 10,000 steps
for i in xrange(10000):
    currentError = trainer.train()
  
  
#save the Neurol network  
   
import pickle
pickle.dump(nn,open(' myneuralnet ' , 'w' ) )
